<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="description" content="UD-VideoQA">
	<meta name="keywords" content="UD-VideoQA">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:image" content="./media/UDVideoQA/teaser.png">
	<meta property="og:url" content="https://UD-VideoQA.github.io/UD-VideoQA/">
	<meta property="og:description" content="Event-based Traffic Monitoring Dataset">

	<title>UD-VideoQA</title>

	<link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
	<link rel="stylesheet" href="./media/UDVideoQA/css/bulma.min.css">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
	<link rel="stylesheet" href="./media/UDVideoQA/css/index.css">
	<link rel="stylesheet" href="./media/UDVideoQA/css/fontawesome.all.min.css">
	<!-- Added Bulma Carousel CSS from CDN -->
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/css/bulma-carousel.min.css">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.5/dist/css/bulma-slider.min.css">

	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
	<script defer src="./media/UDVideoQA/js/fontawesome.all.min.js"></script>
	<!-- Added Bulma Carousel and Slider JS from CDN -->
	<script src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/js/bulma-carousel.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.5/dist/js/bulma-slider.min.js"></script>

	<style>
		.publication-videos {
			display: flex;
			flex-wrap: wrap;
			justify-content: center;
		}

		.video-wrapper {
			flex: 0 0 32%;
			padding: 8px;
		}

		.video-wrapper iframe {
			width: 100%;
			height: 240px;
		}
	</style>
</head>

<body>

	<nav class="navbar" role="navigation" aria-label="main navigation">
		<div class="navbar-brand">
			<a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
				<span aria-hidden="true"></span>
				<span aria-hidden="true"></span>
				<span aria-hidden="true"></span>
			</a>
		</div>

		<div class="navbar-menu">
			<div class="navbar-start" style="flex-grow: 1; justify-content: center;">
				<a class="navbar-item" href="https://UD-VideoQA.github.io/" target="_blank">
					<span class="icon">
						<i class="fas fa-home"></i>
					</span>
				</a>
				<div class="navbar-item has-dropdown is-hoverable">
					<a class="navbar-link" href="#" target="_blank">
						More Research
					</a>
					<div class="navbar-dropdown">
						<a class="navbar-item" href="https://arxiv.org/abs/2412.01132" target="_blank">
							Eyes on the Road: State-of-the-Art VideoQA Models Assessment for Traffic Monitoring Tasks
						</a>
						<div class="navbar-dropdown">
							<a class="navbar-item" href="https://eventbasedvision.github.io/SEVD/" target="_blank">
								SEVD - CVPRW 2024
							</a>
						</div>
					</div>
				</div>
			</div>
		</div>
	</nav>

	<section class="hero">
		<div class="hero-body">
			<div class="container is-max-desktop">
				<div class="column has-text-centered">
					<h1 class="title is-2 publication-title">
						UD-VideoQA : A Benchmark Dataset for Video Question Answering in Traffic Intersection Monitoring
					</h1>

					<div class="is-size-7 publication-authors">
						<p>Paper is Archived</p>
					</div>
					<br>
					<div class="is-size-5 publication-authors">
						<span class="author-block">
							<a href="https://github.com/joe-rabbit">Joseph Raj Vishal</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
						</span>
						<span class="author-block">
							<a href="">Siri</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
						</span>
						<span class="author-block">
							<a href="https://github.com/KathaNaik/">Katha Naik</a>
						</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
						<span class="author-block">
							<a href="https://github.com/patilraje">Rutuja Patil</a>
						</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
						<span class="author-block">
							<a href="">Krishna Vinod</a>
						</span>
						<span class="author-block">
							<a href="">Kashap</a>
						</span>
						<span class="author-block">
							<a href="">Kashap</a>
						</span>
						<span class="author-block">
							<a href="">Pritvi Jai</a>
						</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
						<span class="author-block">
							<a href="https://github.com/UMD-Cognitive-Robot">Yezhou Yang</a>
						</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
						<span class="author-block">
							<a href="https://chakravarthi589.github.io/">Bharatesh Chakravarthi</a>
						</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					</div>

					<div class="is-size-4 publication-authors">
						<span class="author-block">
							<b> Arizona State University </b>
						</span>
					</div>

					<div class="column has-text-centered">
						<div class="publication-links">
							<span class="link-block">
								<a href="https://github.com/joe-rabbit/UDVideoQA_videoqa" target="_blank"
									class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<i class="fab fa-github"></i>
									</span>
									<span>Code</span>
								</a>
							</span>

							<span class="link-block">
								<a href="https://huggingface.co/datasets/joeWabbit/UD_VideoQA_Reasoning_Rich_Video_QA_for_Urban_Traffic"
									target="_blank" class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<i class="far fa-images"></i>
									</span>
									<span>Data</span>
								</a>
							</span>

							<span class="link-block">
								<a href="docs/UD-VideoQADatasetDescription.pdf" target="_blank"
									class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<i class="far fa-file-alt"></i>
									</span>
									<span>Dataset Description</span>
								</a>
							</span>
						</div>
					</div>
				</div>
			</div>
		</div>
	</section>

	<section class="hero teaser">
		<div class="container is-max-desktop">
			<div class="hero-body">
				<img src="./media/UDVideoQA/teaser.png" class="center" />
				<h2 class="subtitles has-text-centered">
					<strong>UD-VideoQA</strong> is a curated, publicly available traffic monitoring dataset gathered using
					ARGOS Cameras and mobile devices under diverse weather and lighting conditions. It comprises 8 hours of
					real-world footage from multiple intersections, segmented into 10-second clips, and features over 25,000
					question-answer pairs covering spatiotemporal dynamics, vehicle interactions, and incident detection. This
					dataset enables the benchmarking and enhancement of VideoQA models for intelligent transportation systems. It
					includes five QA types: (1) attribution, (2) counting, (3) event reasoning, (4) reverse reasoning, and (5)
					counterfactual inference.
				</h2>
			</div>
		</div>
	</section>

	<!-- Restored original Bulma carousel structure -->
	<section class="hero is-small" id="Statistics">
		<div class="hero-body">
			<div class="container">
				<h2 class="title is-3">Dataset Statistics</h2>
				<div id="carousel-demo" class="carousel">
					<div class="item item-1">
						<img src="./media/UDVideoQA/Method.png" 
							alt="RoadSical compared with other datasets"/>
					</div>
					<div class="item item-2">
						<img src="./media/UDVideoQA/Stats.png"
							alt="Road event taxonomy of RoadSocial"/>
					</div>
					<div class="item item-3">
						<img src="./media/UDVideoQA/Intersection.png"
							alt="Road event taxonomy of RoadSocial"/>
					</div>
					<div class="item item-4">
						<img src="./media/UDVideoQA/Tool.png"
							alt="Road event taxonomy of RoadSocial"/>
					</div>
				</div>
			</div>
		</div>
	</section>

	<section class="hero teaser">
		<div class="container is-max-desktop">
			<div class="columns is-centered has-text-centered">
				<div class="column is-four-fifths">
					<h2 class="title is-3">Abstract</h2>
					<div class="content has-text-justified">
						<p>Traffic monitoring is crucial for urban mobility, road safety, and intelligent transportation systems
							(ITS). Deep learning has advanced video-based traffic monitoring through video question answering
							(VideoQA) models, enabling structured insight extraction from traffic videos. However, existing VideoQA
							models struggle with the complexity of real-world traffic scenes, where multiple concurrent events unfold
							across spatiotemporal dimensions. To address these challenges, this paper introduces UD-VideoQA, a
							curated dataset designed to benchmark and enhance VideoQA models for traffic monitoring tasks. The
							UD-VideoQA dataset comprises 8 hours of real-world traffic footage collected from diverse
							intersections, segmented into 10-second video clips, with over 25,000 question-answer (QA) pairs covering
							spatiotemporal dynamics, vehicle interactions, incident detection, and other critical traffic attributes.
							State-of-the-art VideoQA models are evaluated on UD-VideoQA, exposing challenges in reasoning over
							fine-grained spatiotemporal dependencies within complex traffic scenarios. Additionally, fine-tuning these
							models on UD-VideoQA yields notable performance improvements, demonstrating the necessity of
							domain-specific datasets for VideoQA. UD-VideoQA is publicly available as a benchmark dataset to
							facilitate future research in real-world-deployable VideoQA models for intelligent transportation systems.
						</p>
					</div>
				</div>
			</div>
		</div>
	</section>

	<section class="hero teaser">
		<div class="container is-max-desktop">
			<div class="columns is-centered has-text-centered">
				<div class="column is-four-fifths">
					<hr>
					<h2 class="title is-3">UD-VideoQA - Data Collection Setup and Diversity </h2>
					<img src="./media/InterAct/InterAct_Collection.png" class="center" />
					<div class="content has-text-justified">
						<br>
						<p> Overview of the UD-VideoQA data collection framework, integrating traffic video recording and
							processing with a hybrid approach combining manual labeling and GPT-based automation. The pipeline segments eight hours of
							footage into 10-second clips, extracts key metadata (e.g., vehicle attributes, movement patterns,
							pedestrian data), and generates structured question-answer pairs covering attribution, counting, reverse
							reasoning, event reasoning, and counterfactual inference.
						</p>
					</div>
				</div>
			</div>
			<hr>
		</div>
	</section>

	<section class="hero teaser">
		<div class="container is-max-desktop">
			<div class="columns is-centered has-text-centered">
				<div class="column is-four-fifths">
					<h2 class="title is-3">UD-VideoQA - Video Question Generation LeaderBoard </h2>
					<img src="./media/UDVideoQA/Video_Qgen.png" class="center" />
					<div class="content has-text-justified">
						<br>
						<p> Overview of the UD-VideoQA dataset, which comprises 28,800 question-answer pairs across various
							reasoning categories. A higher concentration appears in counting, attribute recognition, and event
							reasoning, followed by counterfactual inference and reverse reasoning (3a). Figures 3(b)-(d) illustrate
							the dataset's emphasis on vehicular-related questions, the dominance of attribution and event reasoning
							categories, and the distribution of question types ("what," "where," and "how"). This structured approach
							supports the analysis of complex, multi-event traffic scenarios, requiring robust spatio-temporal
							reasoning. A rigorous human and GPT-assisted validation process ensures the consistency, accuracy, and
							reliability of all annotations.
						</p>
					</div>
				</div>
			</div>
			<hr>
		</div>
	</section>

	<section class="hero teaser">
		<div class="container is-max-desktop">
			<div class="columns is-centered has-text-centered">
				<div class="column is-four-fifths">
					<h2 class="title is-3">InterAct - Experimentation Statistics </h2>
					<img src="./media/InterAct/InterAct_Model_Stats.png" class="center" />
					<div class="content has-text-justified">
						<br>
						<p> Performance analysis of VideoLlama2, Llava-NeXT-Video, and Qwen2-VL-7B-hf on the InterAct VideoQA
							dataset, highlighting metric distributions (a), before vs. after fine-tuning (b), and multi-metric
							improvements (c). Notably, <strong>Qwen2-VL-7B-hf</strong> demonstrates the most substantial gains across
							complex reasoning tasks, emphasizing the effectiveness of fine-tuning for robust traffic video analysis.
						</p>
					</div>
				</div>
			</div>
			<hr>
		</div>
	</section>

	<section class="hero teaser">
		<div class="container is-max-desktop" style="max-width: 200%; width: 1200px;">
			<div class="hero-body">
				<center>
					<h3 class="title is-3">Sample Data Recordings</h3>
					<div class="publication-videos">
						<div class="video-wrapper">
							<iframe src="https://www.youtube.com/embed/zctaHOSsrXE?si=bF_imjL0QqvdZc1L"
								allow="accelerometer; autoplay; encrypted-media; gyroscope; web-share" allowfullscreen></iframe>
						</div>
						<div class="video-wrapper">
							<iframe src="https://www.youtube.com/embed/cpvGMZYcH24?si=o7q2rowf6SvWgBm-"
								allow="accelerometer; autoplay; encrypted-media; gyroscope; web-share" allowfullscreen></iframe>
						</div>
						<div class="video-wrapper">
							<iframe src="https://www.youtube.com/embed/SSjnZOGdXfM?si=amTy_vwWz5WImBWt"
								allow="accelerometer; autoplay; encrypted-media; gyroscope; web-share" allowfullscreen></iframe>
						</div>
					</div>
				</center>
			</div>
			<hr>
		</div>
	</section>

	<section class="hero teaser" id="BibTeX">
		<div class="container is-max-desktop content">
			<center>
				<h2 class="title">BibTeX</h2>
			</center>
			<pre>
@misc{vishal2025interactvideoreasoningrichvideoqa,
      title={UD-Video: Reasoning-Rich Video QA for Urban Traffic}, 
      author={Joseph Raj Vishal and Divesh Basina and Rutuja Patil and Manas Srinivas Gowda and Katha Naik and Yezhou Yang and Bharatesh Chakravarthi},
      year={2025},
      eprint={2507.14743},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.14743}, 
}
			</pre>
		</div>
	</section>

	<footer class="footer">
		<div class="container">
			<div class="content has-text-centered"></div>
			<div class="columns is-centered">
				<div class="column is-8">
					<div class="content">
						<center>
							<p>
								This website is licensed under a <a rel="license"
									href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
									Commons Attribution-ShareAlike 4.0 International License</a>.
								This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
								We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing
								this template.
							</p>
						</center>
					</div>
				</div>
			</div>
		</div>
	</footer>

	<!-- Integrated JavaScript from on.tsx file -->
	<script>
		window.HELP_IMPROVE_VIDEOJS = false;

		var INTERP_BASE = "./static/interpolation/stacked";
		var NUM_INTERP_FRAMES = 240;

		var interp_images = [];
		function preloadInterpolationImages() {
			for (var i = 0; i < NUM_INTERP_FRAMES; i++) {
				var path = INTERP_BASE + '/' + String(i).padStart(6, '0') + '.jpg';
				interp_images[i] = new Image();
				interp_images[i].src = path;
			}
		}

		function setInterpolationImage(i) {
			var image = interp_images[i];
			image.ondragstart = function() { return false; };
			image.oncontextmenu = function() { return false; };
			$('#interpolation-image-wrapper').empty().append(image);
		}

		$(document).ready(function() {
			// Check for click events on the navbar burger icon
			$(".navbar-burger").click(function() {
				// Toggle the "is-active" class on both the "navbar-burger" and the "navbar-menu"
				$(".navbar-burger").toggleClass("is-active");
				$(".navbar-menu").toggleClass("is-active");
			});

			var options = {
				slidesToScroll: 1,
				slidesToShow: 1,
				loop: true,
				infinite: true,
				autoplay: true,
				autoplaySpeed: 3000,
			}

			// Initialize all div with carousel class
			var carousels = bulmaCarousel.attach('.carousel', options);

			// Loop on each carousel initialized
			for(var i = 0; i < carousels.length; i++) {
				// Add listener to event
				carousels[i].on('before:show', state => {
					console.log(state);
				});
			}

			// Access to bulmaCarousel instance of an element
			var element = document.querySelector('#my-element');
			if (element && element.bulmaCarousel) {
				// bulmaCarousel instance is available as element.bulmaCarousel
				element.bulmaCarousel.on('before-show', function(state) {
					console.log(state);
				});
			}

			// Interpolation slider functionality
			preloadInterpolationImages();

			$('#interpolation-slider').on('input', function(event) {
				setInterpolationImage(this.value);
			});
			setInterpolationImage(0);
			$('#interpolation-slider').prop('max', NUM_INTERP_FRAMES - 1);

			bulmaSlider.attach();
		})
	</script>

</body>

</html>
